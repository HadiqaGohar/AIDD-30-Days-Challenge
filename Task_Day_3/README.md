# 

## üåü PART A ‚Äî Research Questions (Short Answers)

### Question : 1

#### What new improvements were introduced in Gemini 3.0?

Gemini 3.0 brings state-of-the-art reasoning depth, substantially stronger multimodal understanding (text, image, video, audio and code), and new model behaviors like media-resolution handling, ‚Äúthought signatures‚Äù and configurable thinking levels all exposed with updated API parameters.

### Question : 2

#### How does Gemini 3.0 improve coding & automation workflows?

Gemini 3.0 is built for agentic coding: it shows much better tool use (higher Terminal-Bench scores), tighter IDE/agent integration (e.g., Google Antigravity), richer CLI tooling, and API features that let developers run multi-agent workflows, automate testing, and control latency/cost trade-offs. This makes it easier to have AI agents write, run, debug and verify code end-to-end.

### Question : 3

#### How does Gemini 3.0 improve multimodal understanding?

The model fuses and reasons across text, high-resolution images, video and audio in a single pass, producing richer visualizations and interactive outputs (better summaries, scene understanding, transcripts, and analysis of technical media such as medical images or logs). The docs emphasize improved fidelity for media resolution and deeper cross-modal reasoning.

### Question : 4

#### Name any two developer tools introduced with Gemini 3.0.

Two prominent developer tools are Google Antigravity (an agentic development platform/AI-first IDE) and the Gemini CLI (enhanced terminal tooling for agentic coding). The Gemini API/Dev Guide also documents new API features for multi-agent workflows.

---

## üåü PART B ‚Äî Practical Task (Screenshot Required)









